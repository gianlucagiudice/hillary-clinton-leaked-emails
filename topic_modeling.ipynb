{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import regex as re\n",
    "from nltk.stem import PorterStemmer\n",
    "from pandarallel import pandarallel\n",
    "import itertools\n",
    "import collections\n",
    "import subprocess\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "pandarallel.initialize(progress_bar = False)\n",
    "\n",
    "# Serialization folder\n",
    "SERIALIZATION_FOLDER = \"pickle/\"\n",
    "\n",
    "# Serialization folder\n",
    "DF_NAME = \"df.pkl\"\n",
    "\n",
    "# Environment\n",
    "DATA_PATH = 'data/'\n",
    "EMAIL_DATA = 'Emails.csv'\n",
    "\n",
    "ASUM_PATH = 'asum'\n",
    "\n",
    "TOKENS_THLD = 15\n",
    "\n",
    "FREQ_THLD = 10"
   ],
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before: 6737\n",
      "Length after: 1752\n"
     ]
    },
    {
     "data": {
      "text/plain": "    SenderPersonId          MetadataDateSent  \\\nId                                             \n2              NaN 2011-03-03 05:00:00+00:00   \n6             80.0 2012-09-12 04:00:00+00:00   \n14            10.0 2011-03-13 05:00:00+00:00   \n15            32.0 2012-09-12 04:00:00+00:00   \n16            77.0 2012-09-12 04:00:00+00:00   \n\n                                     ExtractedSubject  \\\nId                                                      \n2                                                 NaN   \n6   Meet The Right Wing Extremist Behind Anti-Musl...   \n14                                                NaN   \n15                           RE: Not a dry eye in NEA   \n16                                                NaN   \n\n                                    ExtractedBodyText  DateYear  DateMonth  \\\nId                                                                           \n2   B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest...      2011          3   \n6   Pis print.\\n-•-...-^\\nH < hrod17@clintonernail...      2012          9   \n14  Anne-Marie Slaughter\\nSunday, March 13, 2011 9...      2011          3   \n15  _ .....\\nFrom Randolph, Lawrence M\\nSent: Wedn...      2012          9   \n16  I asked to attend your svtc today with Embassy...      2012          9   \n\n    DateDay                           ExtractedBodyTextCleaned  \\\nId                                                               \n2         3  b6\\nh: latest how syria is aiding qaddafi and ...   \n6        12  °russorv@state.gov'\\nfrom [meat)\\nsent: to: 11...   \n14       13  anne-marie slaughter\\njacob mills, cheryl d; r...   \n15       12  _ .....\\nfrom randolph, lawrence m\\nsent: to: ...   \n16       12  i asked to attend your svtc today with embassy...   \n\n                                            Tokenized  \nId                                                     \n2   [h, latest, syria, aiding, qaddafi, sid, hrc, ...  \n6   [state.gov', meat, sent, subject, meet, right,...  \n14  [anne-marie, slaughter, jacob, mills, cheryl, ...  \n15  [randolph, lawrence, sent, mills, cheryl, subj...  \n16  [asked, attend, svtc, today, embassy, tripoli,...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SenderPersonId</th>\n      <th>MetadataDateSent</th>\n      <th>ExtractedSubject</th>\n      <th>ExtractedBodyText</th>\n      <th>DateYear</th>\n      <th>DateMonth</th>\n      <th>DateDay</th>\n      <th>ExtractedBodyTextCleaned</th>\n      <th>Tokenized</th>\n    </tr>\n    <tr>\n      <th>Id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>2011-03-03 05:00:00+00:00</td>\n      <td>NaN</td>\n      <td>B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest...</td>\n      <td>2011</td>\n      <td>3</td>\n      <td>3</td>\n      <td>b6\\nh: latest how syria is aiding qaddafi and ...</td>\n      <td>[h, latest, syria, aiding, qaddafi, sid, hrc, ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>80.0</td>\n      <td>2012-09-12 04:00:00+00:00</td>\n      <td>Meet The Right Wing Extremist Behind Anti-Musl...</td>\n      <td>Pis print.\\n-•-...-^\\nH &lt; hrod17@clintonernail...</td>\n      <td>2012</td>\n      <td>9</td>\n      <td>12</td>\n      <td>°russorv@state.gov'\\nfrom [meat)\\nsent: to: 11...</td>\n      <td>[state.gov', meat, sent, subject, meet, right,...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>10.0</td>\n      <td>2011-03-13 05:00:00+00:00</td>\n      <td>NaN</td>\n      <td>Anne-Marie Slaughter\\nSunday, March 13, 2011 9...</td>\n      <td>2011</td>\n      <td>3</td>\n      <td>13</td>\n      <td>anne-marie slaughter\\njacob mills, cheryl d; r...</td>\n      <td>[anne-marie, slaughter, jacob, mills, cheryl, ...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>32.0</td>\n      <td>2012-09-12 04:00:00+00:00</td>\n      <td>RE: Not a dry eye in NEA</td>\n      <td>_ .....\\nFrom Randolph, Lawrence M\\nSent: Wedn...</td>\n      <td>2012</td>\n      <td>9</td>\n      <td>12</td>\n      <td>_ .....\\nfrom randolph, lawrence m\\nsent: to: ...</td>\n      <td>[randolph, lawrence, sent, mills, cheryl, subj...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>77.0</td>\n      <td>2012-09-12 04:00:00+00:00</td>\n      <td>NaN</td>\n      <td>I asked to attend your svtc today with Embassy...</td>\n      <td>2012</td>\n      <td>9</td>\n      <td>12</td>\n      <td>i asked to attend your svtc today with embassy...</td>\n      <td>[asked, attend, svtc, today, embassy, tripoli,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(SERIALIZATION_FOLDER + DF_NAME)\n",
    "\n",
    "print(f\"Length before: {len(df)}\")\n",
    "df = df[df[\"Tokenized\"].apply(lambda x: len(x)) > TOKENS_THLD]\n",
    "print(f\"Length after: {len(df)}\")\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, stem=True):\n",
    "    tokenized = word_tokenize(sentence)\n",
    "    # Strip tokens\n",
    "    tokenized = [token.strip() for token in tokenized]\n",
    "    # Strict regex rule\n",
    "    tokenized = [token for token in tokenized if re.match('\\w+', token)]\n",
    "    # Remove punctuation\n",
    "    tokenized = [token for token in tokenized if token not in string.punctuation]\n",
    "    # Remove stopwords\n",
    "    stop = stopwords.words('english') + [':', '.', '@'] + [\"n't\"]\n",
    "    tokenized = [token for token in tokenized if token not in stop]\n",
    "    # Remove numbers\n",
    "    tokenized = [token for token in tokenized if not re.search(r'\\d', token)]\n",
    "    if stem:\n",
    "        tokenized = [stemmer.stem(token) for token in tokenized]\n",
    "    return tokenized\n",
    "\n",
    "def preprocess_asum(body):\n",
    "    sentences = sent_tokenize(body)\n",
    "    sentences = [tokenize_sentence(sentence) for sentence in sentences]\n",
    "    return [s for s in sentences if s != []]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "\"i asked to attend your svtc today with embassy tripoli, because had first met so many of that staff when i went with\\nyou from malta to tripoli for the reopening of our embassy.\\ntoday's deaths hit me much harder than i would have guessed. i am always proud to serve under you, but never have\\nyour words been more meaningful than on today's svtc. every day of your tenure has been extraordinary, but none\\nmore so than today. thank you again for your inspirational leadership and example.\\nas ever,\\nharold\\nu.s. department of state\\ncase no. f-2015-04841\\ndoc no. c05739571\\ndate: 05/13/2015\\nstate dept. - produced to house select benghazi comm.\\nsubject to agreement on sensitive information & redactions. no foia waiver. state-scb0045269\""
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df.iloc[4]['ExtractedBodyTextCleaned']\n",
    "text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "['i asked to attend your svtc today with embassy tripoli, because had first met so many of that staff when i went with\\nyou from malta to tripoli for the reopening of our embassy.',\n \"today's deaths hit me much harder than i would have guessed.\",\n \"i am always proud to serve under you, but never have\\nyour words been more meaningful than on today's svtc.\",\n 'every day of your tenure has been extraordinary, but none\\nmore so than today.',\n 'thank you again for your inspirational leadership and example.',\n 'as ever,\\nharold\\nu.s. department of state\\ncase no.',\n 'f-2015-04841\\ndoc no.',\n 'c05739571\\ndate: 05/13/2015\\nstate dept.',\n '- produced to house select benghazi comm.',\n 'subject to agreement on sensitive information & redactions.',\n 'no foia waiver.',\n 'state-scb0045269']"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_text = nltk.sent_tokenize(text)\n",
    "sent_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "[['ask',\n  'attend',\n  'svtc',\n  'today',\n  'embassi',\n  'tripoli',\n  'first',\n  'met',\n  'mani',\n  'staff',\n  'went',\n  'malta',\n  'tripoli',\n  'reopen',\n  'embassi'],\n ['today', 'death', 'hit', 'much', 'harder', 'would', 'guess'],\n ['alway', 'proud', 'serv', 'never', 'word', 'meaning', 'today', 'svtc'],\n ['everi', 'day', 'tenur', 'extraordinari', 'none', 'today'],\n ['thank', 'inspir', 'leadership', 'exampl'],\n ['ever', 'harold', 'u.s.', 'depart', 'state', 'case'],\n ['doc'],\n ['date', 'state', 'dept'],\n ['produc', 'hous', 'select', 'benghazi', 'comm'],\n ['subject', 'agreement', 'sensit', 'inform', 'redact'],\n ['foia', 'waiver'],\n []]"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenize_sentence(s) for s in sent_text]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "    SenderPersonId          MetadataDateSent  \\\nId                                             \n2              NaN 2011-03-03 05:00:00+00:00   \n6             80.0 2012-09-12 04:00:00+00:00   \n14            10.0 2011-03-13 05:00:00+00:00   \n15            32.0 2012-09-12 04:00:00+00:00   \n16            77.0 2012-09-12 04:00:00+00:00   \n\n                                     ExtractedSubject  \\\nId                                                      \n2                                                 NaN   \n6   Meet The Right Wing Extremist Behind Anti-Musl...   \n14                                                NaN   \n15                           RE: Not a dry eye in NEA   \n16                                                NaN   \n\n                                    ExtractedBodyText  DateYear  DateMonth  \\\nId                                                                           \n2   B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest...      2011          3   \n6   Pis print.\\n-•-...-^\\nH < hrod17@clintonernail...      2012          9   \n14  Anne-Marie Slaughter\\nSunday, March 13, 2011 9...      2011          3   \n15  _ .....\\nFrom Randolph, Lawrence M\\nSent: Wedn...      2012          9   \n16  I asked to attend your svtc today with Embassy...      2012          9   \n\n    DateDay                           ExtractedBodyTextCleaned  \\\nId                                                               \n2         3  b6\\nh: latest how syria is aiding qaddafi and ...   \n6        12  °russorv@state.gov'\\nfrom [meat)\\nsent: to: 11...   \n14       13  anne-marie slaughter\\njacob mills, cheryl d; r...   \n15       12  _ .....\\nfrom randolph, lawrence m\\nsent: to: ...   \n16       12  i asked to attend your svtc today with embassy...   \n\n                                            Tokenized  \\\nId                                                      \n2   [h, latest, syria, aiding, qaddafi, sid, hrc, ...   \n6   [state.gov', meat, sent, subject, meet, right,...   \n14  [anne-marie, slaughter, jacob, mills, cheryl, ...   \n15  [randolph, lawrence, sent, mills, cheryl, subj...   \n16  [asked, attend, svtc, today, embassy, tripoli,...   \n\n                                                 asum  \nId                                                     \n2   [[h, latest, syria, aid, qaddafi, sid, hrc, me...  \n6   [[state.gov', meat, sent, subject, meet, right...  \n14  [[anne-mari, slaughter, jacob, mill, cheryl, r...  \n15  [[randolph, lawrenc, sent, mill, cheryl, subje...  \n16  [[ask, attend, svtc, today, embassi, tripoli, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SenderPersonId</th>\n      <th>MetadataDateSent</th>\n      <th>ExtractedSubject</th>\n      <th>ExtractedBodyText</th>\n      <th>DateYear</th>\n      <th>DateMonth</th>\n      <th>DateDay</th>\n      <th>ExtractedBodyTextCleaned</th>\n      <th>Tokenized</th>\n      <th>asum</th>\n    </tr>\n    <tr>\n      <th>Id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>2011-03-03 05:00:00+00:00</td>\n      <td>NaN</td>\n      <td>B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest...</td>\n      <td>2011</td>\n      <td>3</td>\n      <td>3</td>\n      <td>b6\\nh: latest how syria is aiding qaddafi and ...</td>\n      <td>[h, latest, syria, aiding, qaddafi, sid, hrc, ...</td>\n      <td>[[h, latest, syria, aid, qaddafi, sid, hrc, me...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>80.0</td>\n      <td>2012-09-12 04:00:00+00:00</td>\n      <td>Meet The Right Wing Extremist Behind Anti-Musl...</td>\n      <td>Pis print.\\n-•-...-^\\nH &lt; hrod17@clintonernail...</td>\n      <td>2012</td>\n      <td>9</td>\n      <td>12</td>\n      <td>°russorv@state.gov'\\nfrom [meat)\\nsent: to: 11...</td>\n      <td>[state.gov', meat, sent, subject, meet, right,...</td>\n      <td>[[state.gov', meat, sent, subject, meet, right...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>10.0</td>\n      <td>2011-03-13 05:00:00+00:00</td>\n      <td>NaN</td>\n      <td>Anne-Marie Slaughter\\nSunday, March 13, 2011 9...</td>\n      <td>2011</td>\n      <td>3</td>\n      <td>13</td>\n      <td>anne-marie slaughter\\njacob mills, cheryl d; r...</td>\n      <td>[anne-marie, slaughter, jacob, mills, cheryl, ...</td>\n      <td>[[anne-mari, slaughter, jacob, mill, cheryl, r...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>32.0</td>\n      <td>2012-09-12 04:00:00+00:00</td>\n      <td>RE: Not a dry eye in NEA</td>\n      <td>_ .....\\nFrom Randolph, Lawrence M\\nSent: Wedn...</td>\n      <td>2012</td>\n      <td>9</td>\n      <td>12</td>\n      <td>_ .....\\nfrom randolph, lawrence m\\nsent: to: ...</td>\n      <td>[randolph, lawrence, sent, mills, cheryl, subj...</td>\n      <td>[[randolph, lawrenc, sent, mill, cheryl, subje...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>77.0</td>\n      <td>2012-09-12 04:00:00+00:00</td>\n      <td>NaN</td>\n      <td>I asked to attend your svtc today with Embassy...</td>\n      <td>2012</td>\n      <td>9</td>\n      <td>12</td>\n      <td>i asked to attend your svtc today with embassy...</td>\n      <td>[asked, attend, svtc, today, embassy, tripoli,...</td>\n      <td>[[ask, attend, svtc, today, embassi, tripoli, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"asum\"] = df[\"ExtractedBodyTextCleaned\"].parallel_apply(preprocess_asum)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe tokens frequency\n"
     ]
    },
    {
     "data": {
      "text/plain": "count    20088.000000\nmean        13.966697\nstd         57.815385\nmin          1.000000\n25%          1.000000\n50%          2.000000\n75%          6.000000\nmax       2122.000000\ndtype: float64"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = list(itertools.chain(*list(itertools.chain(*df[\"asum\"]))))\n",
    "tokens_freq = collections.Counter(flatten)\n",
    "print(\"Describe tokens frequency\")\n",
    "pd.Series(tokens_freq.values()).describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After thld on frequency: 3419\n"
     ]
    }
   ],
   "source": [
    "top_tokens = [token for token, occ in tokens_freq.items() if occ > FREQ_THLD]\n",
    "print(f\"After thld on frequency: {len(top_tokens)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "{'abandon': 0,\n 'abba': 1,\n 'abc': 2,\n 'abedin': 3,\n 'abil': 4,\n 'abl': 5,\n 'abort': 6,\n 'abraham': 7,\n 'abroad': 8,\n 'absenc': 9,\n 'absolut': 10,\n 'abu': 11,\n 'abus': 12,\n 'academ': 13,\n 'academi': 14,\n 'acceler': 15,\n 'accept': 16,\n 'access': 17,\n 'accommod': 18,\n 'accompani': 19,\n 'accomplish': 20,\n 'accord': 21,\n 'accordingli': 22,\n 'account': 23,\n 'accur': 24,\n 'accus': 25,\n 'achiev': 26,\n 'acknowledg': 27,\n 'acorn': 28,\n 'across': 29,\n 'act': 30,\n 'action': 31,\n 'activ': 32,\n 'activist': 33,\n 'actor': 34,\n 'actual': 35,\n 'ad': 36,\n 'adam': 37,\n 'adapt': 38,\n 'add': 39,\n 'addit': 40,\n 'address': 41,\n 'adequ': 42,\n 'adjust': 43,\n 'administ': 44,\n 'administr': 45,\n 'admir': 46,\n 'admit': 47,\n 'adopt': 48,\n 'adult': 49,\n 'advanc': 50,\n 'advantag': 51,\n 'adversari': 52,\n 'advertis': 53,\n 'advic': 54,\n 'advis': 55,\n 'advisor': 56,\n 'advisori': 57,\n 'advoc': 58,\n 'advocaci': 59,\n 'afb': 60,\n 'affair': 61,\n 'affect': 62,\n 'affili': 63,\n 'affirm': 64,\n 'afford': 65,\n 'afghan': 66,\n 'afghanistan': 67,\n 'afp': 68,\n 'africa': 69,\n 'african': 70,\n 'aftermath': 71,\n 'afternoon': 72,\n 'afterward': 73,\n 'age': 74,\n 'agenc': 75,\n 'agenda': 76,\n 'agent': 77,\n 'aggress': 78,\n 'ago': 79,\n 'agre': 80,\n 'agreement': 81,\n 'agricultur': 82,\n 'ahead': 83,\n 'ahmadinejad': 84,\n 'aid': 85,\n 'aim': 86,\n 'aipac': 87,\n 'air': 88,\n 'aircraft': 89,\n 'airport': 90,\n 'airway': 91,\n 'aisl': 92,\n 'al': 93,\n 'alarm': 94,\n 'albright': 95,\n 'alert': 96,\n 'alexand': 97,\n 'ali': 98,\n 'alien': 99,\n 'alik': 100,\n 'alleg': 101,\n 'alli': 102,\n 'allianc': 103,\n 'alloc': 104,\n 'allow': 105,\n 'almost': 106,\n 'alon': 107,\n 'along': 108,\n 'alongsid': 109,\n 'alreadi': 110,\n 'also': 111,\n 'alter': 112,\n 'altern': 113,\n 'although': 114,\n 'alway': 115,\n 'am': 116,\n 'amaz': 117,\n 'amazon': 118,\n 'amb': 119,\n 'ambassador': 120,\n 'ambigu': 121,\n 'ambit': 122,\n 'ambiti': 123,\n 'amend': 124,\n 'america': 125,\n 'american': 126,\n 'amid': 127,\n 'among': 128,\n 'amorim': 129,\n 'amount': 130,\n 'amplifi': 131,\n 'analysi': 132,\n 'analyst': 133,\n 'andrew': 134,\n 'angel': 135,\n 'anger': 136,\n 'angl': 137,\n 'angri': 138,\n 'anim': 139,\n 'ann': 140,\n 'anniversari': 141,\n 'announc': 142,\n 'annual': 143,\n 'anonym': 144,\n 'anoth': 145,\n 'ansar': 146,\n 'answer': 147,\n 'anticip': 148,\n 'anxieti': 149,\n 'anybodi': 150,\n 'anyon': 151,\n 'anyth': 152,\n 'anyway': 153,\n 'ap': 154,\n 'apart': 155,\n 'apolog': 156,\n 'apologis': 157,\n 'appar': 158,\n 'apparatu': 159,\n 'appeal': 160,\n 'appear': 161,\n 'appli': 162,\n 'applic': 163,\n 'appoint': 164,\n 'appreci': 165,\n 'approach': 166,\n 'appropri': 167,\n 'approv': 168,\n 'approx': 169,\n 'approxim': 170,\n 'april': 171,\n 'aq': 172,\n 'arab': 173,\n 'arabia': 174,\n 'arc': 175,\n 'area': 176,\n 'argentina': 177,\n 'argu': 178,\n 'argument': 179,\n 'arizona': 180,\n 'arkin': 181,\n 'arm': 182,\n 'armey': 183,\n 'armi': 184,\n 'around': 185,\n 'arrang': 186,\n 'array': 187,\n 'arrest': 188,\n 'arriv': 189,\n 'arrog': 190,\n 'arsen': 191,\n 'art': 192,\n 'articl': 193,\n 'articul': 194,\n 'arturo': 195,\n 'asean': 196,\n 'ashton': 197,\n 'asia': 198,\n 'asian': 199,\n 'asid': 200,\n 'ask': 201,\n 'aspect': 202,\n 'aspir': 203,\n 'assang': 204,\n 'assassin': 205,\n 'assault': 206,\n 'assembl': 207,\n 'assert': 208,\n 'assess': 209,\n 'asset': 210,\n 'assign': 211,\n 'assist': 212,\n 'associ': 213,\n 'assum': 214,\n 'assur': 215,\n 'atlant': 216,\n 'atmospher': 217,\n 'attach': 218,\n 'attack': 219,\n 'attempt': 220,\n 'attend': 221,\n 'attent': 222,\n 'attitud': 223,\n 'attorney': 224,\n 'attract': 225,\n 'audienc': 226,\n 'auditorium': 227,\n 'august': 228,\n 'australia': 229,\n 'author': 230,\n 'authoritarian': 231,\n 'avail': 232,\n 'avenu': 233,\n 'averag': 234,\n 'avoid': 235,\n 'await': 236,\n 'awar': 237,\n 'award': 238,\n 'away': 239,\n 'axelrod': 240,\n 'azerbaijan': 241,\n 'b': 242,\n 'babi': 243,\n 'back': 244,\n 'background': 245,\n 'bad': 246,\n 'badli': 247,\n 'baghdad': 248,\n 'bahtiyar': 249,\n 'bailout': 250,\n 'baker': 251,\n 'balanc': 252,\n 'balkan': 253,\n 'ball': 254,\n 'ballot': 255,\n 'ban': 256,\n 'band': 257,\n 'bank': 258,\n 'banker': 259,\n 'bar': 260,\n 'barack': 261,\n 'barak': 262,\n 'barbara': 263,\n 'bare': 264,\n 'base': 265,\n 'basi': 266,\n 'basic': 267,\n 'battl': 268,\n 'baucu': 269,\n 'bauer': 270,\n 'bay': 271,\n 'bbc': 272,\n 'bear': 273,\n 'beast': 274,\n 'beat': 275,\n 'beaten': 276,\n 'becam': 277,\n 'beck': 278,\n 'becom': 279,\n 'began': 280,\n 'begin': 281,\n 'begun': 282,\n 'behalf': 283,\n 'behav': 284,\n 'behavior': 285,\n 'behind': 286,\n 'beij': 287,\n 'belfast': 288,\n 'belief': 289,\n 'believ': 290,\n 'belong': 291,\n 'ben': 292,\n 'benefit': 293,\n 'benghazi': 294,\n 'benjamin': 295,\n 'berlin': 296,\n 'besid': 297,\n 'best': 298,\n 'bet': 299,\n 'betray': 300,\n 'better': 301,\n 'beyond': 302,\n 'bhutto': 303,\n 'bibi': 304,\n 'bid': 305,\n 'biden': 306,\n 'big': 307,\n 'bigger': 308,\n 'biggest': 309,\n 'bilat': 310,\n 'bilater': 311,\n 'bill': 312,\n 'billion': 313,\n 'billionair': 314,\n 'bin': 315,\n 'bipartisan': 316,\n 'birch': 317,\n 'birth': 318,\n 'bit': 319,\n 'black': 320,\n 'blackberri': 321,\n 'blair': 322,\n 'blame': 323,\n 'blast': 324,\n 'bless': 325,\n 'blind': 326,\n 'block': 327,\n 'blog': 328,\n 'blogger': 329,\n 'bloomberg': 330,\n 'blow': 331,\n 'blue': 332,\n 'blumenth': 333,\n 'board': 334,\n 'bob': 335,\n 'bodi': 336,\n 'boehner': 337,\n 'bold': 338,\n 'bomb': 339,\n 'bomber': 340,\n 'bond': 341,\n 'bonus': 342,\n 'book': 343,\n 'boost': 344,\n 'border': 345,\n 'born': 346,\n 'boss': 347,\n 'boston': 348,\n 'bottom': 349,\n 'bought': 350,\n 'bound': 351,\n 'box': 352,\n 'boy': 353,\n 'brainard': 354,\n 'branch': 355,\n 'brand': 356,\n 'brave': 357,\n 'brazil': 358,\n 'brazilian': 359,\n 'break': 360,\n 'breakfast': 361,\n 'breakthrough': 362,\n 'brian': 363,\n 'bridg': 364,\n 'brief': 365,\n 'briefli': 366,\n 'brilliant': 367,\n 'bring': 368,\n 'britain': 369,\n 'british': 370,\n 'broad': 371,\n 'broadcast': 372,\n 'broader': 373,\n 'brock': 374,\n 'broke': 375,\n 'broken': 376,\n 'broker': 377,\n 'brook': 378,\n 'brother': 379,\n 'brought': 380,\n 'brown': 381,\n 'bruce': 382,\n 'brussel': 383,\n 'brutal': 384,\n 'bu': 385,\n 'buckley': 386,\n 'budget': 387,\n 'build': 388,\n 'built': 389,\n 'burden': 390,\n 'bureau': 391,\n 'bureaucraci': 392,\n 'bureaucrat': 393,\n 'burma': 394,\n 'burn': 395,\n 'bush': 396,\n 'busi': 397,\n 'buy': 398,\n 'c': 399,\n 'ca': 400,\n 'cabinet': 401,\n 'cabl': 402,\n 'cairo': 403,\n 'calcul': 404,\n 'california': 405,\n 'call': 406,\n 'came': 407,\n 'camera': 408,\n 'cameron': 409,\n 'camp': 410,\n 'campaign': 411,\n 'campbel': 412,\n 'campu': 413,\n 'canada': 414,\n 'cancel': 415,\n 'cancer': 416,\n 'candid': 417,\n 'cap': 418,\n 'capabl': 419,\n 'capac': 420,\n 'capit': 421,\n 'capitol': 422,\n 'captur': 423,\n 'car': 424,\n 'card': 425,\n 'care': 426,\n 'career': 427,\n 'carleto': 428,\n 'carlo': 429,\n 'carolina': 430,\n 'carri': 431,\n 'carter': 432,\n 'case': 433,\n 'cash': 434,\n 'cast': 435,\n 'casualti': 436,\n 'catastroph': 437,\n 'catch': 438,\n 'cathol': 439,\n 'cato': 440,\n 'caucu': 441,\n 'caught': 442,\n 'caus': 443,\n 'caution': 444,\n 'cb': 445,\n 'cc': 446,\n 'cdm': 447,\n 'celebr': 448,\n 'cell': 449,\n 'cent': 450,\n 'centcom': 451,\n 'center': 452,\n 'centr': 453,\n 'central': 454,\n 'centrist': 455,\n 'centuri': 456,\n 'ceo': 457,\n 'ceremoni': 458,\n 'certain': 459,\n 'certainli': 460,\n 'chair': 461,\n 'chairman': 462,\n 'chairwoman': 463,\n 'challeng': 464,\n 'chamber': 465,\n 'champion': 466,\n 'chanc': 467,\n 'chang': 468,\n 'channel': 469,\n 'chao': 470,\n 'chapter': 471,\n 'charact': 472,\n 'character': 473,\n 'charg': 474,\n 'charl': 475,\n 'chart': 476,\n 'charter': 477,\n 'chase': 478,\n 'chat': 479,\n 'chavez': 480,\n 'check': 481,\n 'cheer': 482,\n 'chelsea': 483,\n 'chemic': 484,\n 'cheney': 485,\n 'cheri': 486,\n 'cheryl': 487,\n 'chicago': 488,\n 'chief': 489,\n 'child': 490,\n 'children': 491,\n 'chile': 492,\n 'china': 493,\n 'chines': 494,\n 'choic': 495,\n 'choos': 496,\n 'chose': 497,\n 'chri': 498,\n 'christian': 499,\n 'christma': 500,\n 'christoph': 501,\n 'church': 502,\n 'cia': 503,\n 'cingular': 504,\n 'circl': 505,\n 'circul': 506,\n 'circumst': 507,\n 'cite': 508,\n 'citi': 509,\n 'citizen': 510,\n 'citizenship': 511,\n 'civic': 512,\n 'civil': 513,\n 'civilian': 514,\n 'claim': 515,\n 'clair': 516,\n 'clarifi': 517,\n 'clash': 518,\n 'class': 519,\n 'classifi': 520,\n 'clean': 521,\n 'clear': 522,\n 'clearli': 523,\n 'clegg': 524,\n 'clerk': 525,\n 'click': 526,\n 'client': 527,\n 'climat': 528,\n 'clinton': 529,\n 'clip': 530,\n 'close': 531,\n 'closer': 532,\n 'closest': 533,\n 'clotur': 534,\n 'club': 535,\n 'cnn': 536,\n 'co': 537,\n 'coalit': 538,\n 'coat': 539,\n 'cohen': 540,\n 'coin': 541,\n 'cold': 542,\n 'collabor': 543,\n 'collaps': 544,\n 'colleagu': 545,\n 'collect': 546,\n 'colleg': 547,\n 'collin': 548,\n 'colombia': 549,\n 'colombian': 550,\n 'colonel': 551,\n 'color': 552,\n 'columbia': 553,\n 'column': 554,\n 'columnist': 555,\n 'combat': 556,\n 'combin': 557,\n 'come': 558,\n 'comfort': 559,\n 'comm': 560,\n 'command': 561,\n 'comment': 562,\n 'commentari': 563,\n 'commerc': 564,\n 'commerci': 565,\n 'commiss': 566,\n 'commission': 567,\n 'commit': 568,\n 'committe': 569,\n 'common': 570,\n 'commun': 571,\n 'communist': 572,\n 'compact': 573,\n 'compani': 574,\n 'compar': 575,\n 'compel': 576,\n 'compet': 577,\n 'competit': 578,\n 'complain': 579,\n 'complaint': 580,\n 'complet': 581,\n 'complex': 582,\n 'complic': 583,\n 'compon': 584,\n 'compound': 585,\n 'comprehens': 586,\n 'compromis': 587,\n 'comput': 588,\n 'conced': 589,\n 'conceiv': 590,\n 'concentr': 591,\n 'concept': 592,\n 'concern': 593,\n 'concess': 594,\n 'conclud': 595,\n 'conclus': 596,\n 'concret': 597,\n 'condemn': 598,\n 'condit': 599,\n 'condol': 600,\n 'conduct': 601,\n 'conf': 602,\n 'confer': 603,\n 'confess': 604,\n 'confid': 605,\n 'confidenti': 606,\n 'confin': 607,\n 'confirm': 608,\n 'conflict': 609,\n 'confront': 610,\n 'confus': 611,\n 'congo': 612,\n 'congratul': 613,\n 'congress': 614,\n 'congression': 615,\n 'congressman': 616,\n 'connect': 617,\n 'consensu': 618,\n 'consent': 619,\n 'consequ': 620,\n 'conserv': 621,\n 'consid': 622,\n 'consider': 623,\n 'consist': 624,\n 'conspiraci': 625,\n 'constant': 626,\n 'constantli': 627,\n 'constitu': 628,\n 'constitut': 629,\n 'construct': 630,\n 'consul': 631,\n 'consult': 632,\n 'consum': 633,\n 'contact': 634,\n 'contain': 635,\n 'contend': 636,\n 'content': 637,\n 'contest': 638,\n 'context': 639,\n 'conting': 640,\n 'continu': 641,\n 'contract': 642,\n 'contrari': 643,\n 'contrast': 644,\n 'contribut': 645,\n 'contributor': 646,\n 'control': 647,\n 'controversi': 648,\n 'conven': 649,\n 'conveni': 650,\n 'convent': 651,\n 'convers': 652,\n 'convey': 653,\n 'convict': 654,\n 'convinc': 655,\n 'convoy': 656,\n 'cooper': 657,\n 'coordin': 658,\n 'copenhagen': 659,\n 'copi': 660,\n 'copyright': 661,\n 'core': 662,\n 'corker': 663,\n 'corner': 664,\n 'corp': 665,\n 'corpor': 666,\n 'correct': 667,\n 'correspond': 668,\n 'corrupt': 669,\n 'cost': 670,\n 'could': 671,\n 'council': 672,\n 'counsel': 673,\n 'count': 674,\n 'counter': 675,\n 'counterinsurg': 676,\n 'counterpart': 677,\n 'counterterror': 678,\n 'counti': 679,\n 'countri': 680,\n 'coup': 681,\n 'coupl': 682,\n 'courag': 683,\n 'cours': 684,\n 'court': 685,\n 'cover': 686,\n 'coverag': 687,\n 'cr': 688,\n 'crack': 689,\n 'craft': 690,\n 'craig': 691,\n 'crazi': 692,\n 'creat': 693,\n 'creation': 694,\n 'creativ': 695,\n 'credibl': 696,\n 'credit': 697,\n 'crime': 698,\n 'crimin': 699,\n 'crisi': 700,\n 'critic': 701,\n 'crocker': 702,\n 'crop': 703,\n 'cross': 704,\n 'crowd': 705,\n 'crucial': 706,\n 'crusad': 707,\n 'crush': 708,\n 'cuba': 709,\n 'cultur': 710,\n 'curb': 711,\n 'currenc': 712,\n 'current': 713,\n 'custom': 714,\n 'cut': 715,\n 'cvc': 716,\n 'cycl': 717,\n 'cynic': 718,\n 'czech': 719,\n 'daili': 720,\n 'damag': 721,\n 'dan': 722,\n 'danger': 723,\n 'daniel': 724,\n 'dare': 725,\n 'dark': 726,\n 'data': 727,\n 'date': 728,\n 'daughter': 729,\n 'davi': 730,\n 'david': 731,\n 'davutoglu': 732,\n 'day': 733,\n 'dc': 734,\n 'dca': 735,\n 'de': 736,\n 'dead': 737,\n 'deadlin': 738,\n 'deal': 739,\n 'dealt': 740,\n 'dean': 741,\n 'dear': 742,\n 'death': 743,\n 'debat': 744,\n 'debt': 745,\n 'dec': 746,\n 'decad': 747,\n 'decemb': 748,\n 'decid': 749,\n 'decis': 750,\n 'declan': 751,\n 'declar': 752,\n 'declin': 753,\n 'dedic': 754,\n 'deed': 755,\n 'deep': 756,\n 'deepli': 757,\n 'defam': 758,\n 'defeat': 759,\n 'defenc': 760,\n 'defend': 761,\n 'defens': 762,\n 'deficit': 763,\n 'defin': 764,\n 'definit': 765,\n 'degre': 766,\n 'delay': 767,\n 'deleg': 768,\n 'delhi': 769,\n 'deliber': 770,\n 'deliv': 771,\n 'deliveri': 772,\n 'dem': 773,\n 'demand': 774,\n 'demint': 775,\n 'democraci': 776,\n 'democrat': 777,\n 'demonstr': 778,\n 'deni': 779,\n 'denni': 780,\n 'denounc': 781,\n 'depart': 782,\n 'departur': 783,\n 'depend': 784,\n 'depict': 785,\n 'deplor': 786,\n 'deploy': 787,\n 'deposit': 788,\n 'depress': 789,\n 'dept': 790,\n 'depth': 791,\n 'deputi': 792,\n 'derek': 793,\n 'descend': 794,\n 'describ': 795,\n 'deserv': 796,\n 'design': 797,\n 'desir': 798,\n 'desk': 799,\n 'desper': 800,\n 'despit': 801,\n 'destroy': 802,\n 'destruct': 803,\n 'detail': 804,\n 'detain': 805,\n 'detaine': 806,\n 'detent': 807,\n 'determin': 808,\n 'deuel': 809,\n 'devast': 810,\n 'develop': 811,\n 'devic': 812,\n 'devolut': 813,\n 'devolv': 814,\n 'devot': 815,\n 'dialogu': 816,\n 'dick': 817,\n 'dictat': 818,\n 'die': 819,\n 'differ': 820,\n 'difficult': 821,\n 'difficulti': 822,\n 'digniti': 823,\n 'dimens': 824,\n 'dine': 825,\n 'dinner': 826,\n 'diplomaci': 827,\n 'diplomat': 828,\n 'direct': 829,\n 'directli': 830,\n 'director': 831,\n 'dirksen': 832,\n 'disabl': 833,\n 'disagr': 834,\n 'disagre': 835,\n 'disappear': 836,\n 'disappoint': 837,\n 'disapprov': 838,\n 'disast': 839,\n 'disastr': 840,\n 'disciplin': 841,\n 'disclos': 842,\n 'discov': 843,\n 'discrimin': 844,\n 'discuss': 845,\n 'dismantl': 846,\n 'dismiss': 847,\n 'display': 848,\n 'disproportion': 849,\n 'disput': 850,\n 'disrupt': 851,\n 'dissatisfi': 852,\n 'dissid': 853,\n 'distinct': 854,\n 'distinguish': 855,\n 'distribut': 856,\n 'district': 857,\n 'disturb': 858,\n 'divers': 859,\n 'divid': 860,\n 'divis': 861,\n 'divorc': 862,\n 'do': 863,\n 'dobbin': 864,\n 'dobson': 865,\n 'doc': 866,\n 'doctor': 867,\n 'doctrin': 868,\n 'document': 869,\n 'dod': 870,\n 'dodd': 871,\n 'dog': 872,\n 'doherti': 873,\n 'dollar': 874,\n 'domenici': 875,\n 'domest': 876,\n 'domin': 877,\n 'donat': 878,\n 'done': 879,\n 'donilon': 880,\n 'donor': 881,\n 'door': 882,\n 'doubl': 883,\n 'doubt': 884,\n 'doug': 885,\n 'dougla': 886,\n 'down': 887,\n 'download': 888,\n 'dozen': 889,\n 'dr': 890,\n 'draft': 891,\n 'drama': 892,\n 'dramat': 893,\n 'drastic': 894,\n 'draw': 895,\n 'drawn': 896,\n 'dream': 897,\n 'dress': 898,\n 'drew': 899,\n 'drink': 900,\n 'drive': 901,\n 'drop': 902,\n 'drug': 903,\n 'due': 904,\n 'dump': 905,\n 'dup': 906,\n 'durbin': 907,\n 'dutch': 908,\n 'duti': 909,\n 'dynam': 910,\n 'dysfunct': 911,\n 'e': 912,\n 'ea': 913,\n 'eager': 914,\n 'earli': 915,\n 'earlier': 916,\n 'earn': 917,\n 'earth': 918,\n 'earthquak': 919,\n 'easi': 920,\n 'easier': 921,\n 'easili': 922,\n 'east': 923,\n 'eastern': 924,\n 'echo': 925,\n 'econom': 926,\n 'economi': 927,\n 'economist': 928,\n 'ed': 929,\n 'edg': 930,\n 'edit': 931,\n 'editor': 932,\n 'editori': 933,\n 'edt': 934,\n 'educ': 935,\n 'edward': 936,\n 'effect': 937,\n 'effici': 938,\n 'effort': 939,\n 'egypt': 940,\n 'egyptian': 941,\n 'ehud': 942,\n 'eight': 943,\n 'eikenberri': 944,\n 'either': 945,\n 'el': 946,\n 'elect': 947,\n 'elector': 948,\n 'electr': 949,\n 'element': 950,\n 'elev': 951,\n 'elimin': 952,\n 'elit': 953,\n 'ellen': 954,\n 'els': 955,\n 'elsewher': 956,\n 'email': 957,\n 'emanuel': 958,\n 'embarrass': 959,\n 'embassi': 960,\n 'embrac': 961,\n 'emerg': 962,\n 'emiss': 963,\n 'emot': 964,\n 'empey': 965,\n 'emphas': 966,\n 'emphasi': 967,\n 'empir': 968,\n 'employ': 969,\n 'employe': 970,\n 'empow': 971,\n 'en': 972,\n 'enabl': 973,\n 'enact': 974,\n 'encount': 975,\n 'encourag': 976,\n 'end': 977,\n 'endang': 978,\n 'endless': 979,\n 'endors': 980,\n 'endow': 981,\n 'endur': 982,\n 'enemi': 983,\n 'energi': 984,\n 'enforc': 985,\n 'engag': 986,\n 'engin': 987,\n 'english': 988,\n 'enhanc': 989,\n 'enjoy': 990,\n 'enorm': 991,\n 'enough': 992,\n 'ensur': 993,\n 'enter': 994,\n 'enterpris': 995,\n 'enthusiasm': 996,\n 'enthusiast': 997,\n 'entir': 998,\n 'entiti': 999,\n ...}"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tokens = [token for token in top_tokens if re.match(\"^[a-z]+$\", token)]\n",
    "top_tokens = set(top_tokens) - {'url', 'http'}\n",
    "top_tokens = sorted(top_tokens)\n",
    "top_tokens = {idx: token for idx, token in zip(top_tokens, range(len(top_tokens)))}\n",
    "top_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def resolve_asum(sentences):\n",
    "    # Resolve indices\n",
    "    resolved = []\n",
    "    for sentence in sentences:\n",
    "        resolved.append([top_tokens.get(token, None) for token in sentence])\n",
    "    # Remove None values\n",
    "    no_none = []\n",
    "    for l in resolved:\n",
    "        no_none.append([x for x in l if x is not None])\n",
    "    # Remove empty list\n",
    "    out_list = []\n",
    "    for l in no_none:\n",
    "        out_list.append([x for x in l if x != []])\n",
    "    # Remove empty body\n",
    "    return [x for x in out_list if x != []]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "Id\n2       [[1336, 1695, 2945, 85, 2388, 2727, 1428, 1875...\n6       [[2677, 2898, 1871, 2567, 3283, 1087, 286, 114...\n14      [[2756, 1579, 1906, 487, 3, 2225, 1745, 161, 2...\n15      [[2677, 1906, 487, 2898, 1477, 1911], [2507, 2...\n16      [[201, 221, 3039, 960, 3092, 1158, 1886, 1820,...\n                              ...                        \n7934    [[933, 161, 330, 2083, 2321, 1868, 2355, 1477,...\n7938    [[748, 1004, 2273, 1691, 2016, 3202, 1719, 140...\n7939    [[2792, 467, 2663, 3226, 1371, 830, 1309, 2549...\n7942    [[307, 468, 2235, 2672], [2672, 2490, 142, 177...\n7944    [[2386, 2241, 1732, 1660, 1380, 782, 1353, 224...\nName: asum, Length: 1752, dtype: object"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_series = df[\"asum\"].parallel_apply(resolve_asum)\n",
    "final_series\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# Wordlist\n",
    "with open(os.path.join(ASUM_PATH,'in', 'WordList.txt'), \"w+\") as f:\n",
    "    for word in top_tokens.keys():\n",
    "        f.write(word + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# Bag of sentences\n",
    "with open(os.path.join(ASUM_PATH, 'in', 'BagOfSentences.txt'), \"w+\") as f:\n",
    "    for senteces in final_series:\n",
    "        f.write(str(len(senteces)) + \"\\n\")\n",
    "        for sentence in senteces:\n",
    "            f.write(' '.join([str(x) for x in sentence]) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "'java asum.bin.sto2.STO2Core -s 2 -t 10 -i 1000 -th 12 -a 0.1 -b 0.001/0.1/0.1 -g 1/1 -d asum/in -o asum/out'"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_SENTIMENT = '2'\n",
    "N_TOPICS = '10'\n",
    "N_ITER = '1000'\n",
    "N_THREAD = str(psutil.cpu_count())\n",
    "ALPHA = '0.1'\n",
    "BETHA = '0.001/0.1/0.1'\n",
    "G = '1/1'\n",
    "SCRIPT = '.'.join([ASUM_PATH, 'bin', 'sto2.STO2Core'])\n",
    "INPUT = os.path.join(ASUM_PATH, 'in')\n",
    "OUTPUT = os.path.join(ASUM_PATH, 'out')\n",
    "\n",
    "\n",
    "COMMAND = f'java {SCRIPT} -s {N_SENTIMENT} -t {N_TOPICS} -i {N_ITER} ' \\\n",
    "          f'-th {N_THREAD} -a {ALPHA} -b {BETHA} -g {G} -d {INPUT} -o {OUTPUT}'\n",
    "COMMAND"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "#o = subprocess.run(COMMAND, shell=True, capture_output=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}